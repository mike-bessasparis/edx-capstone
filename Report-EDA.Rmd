---
title: "Data Science: Capstone Report"
output: html_notebook
---

## Executive Summary
Data Science Course Colleagues, the goal of this project is to accurately predict the rating users will give to movies with a RMSE less than 0.87750.  This is the motivation to reinforce past learning and to explorer solutions to real-world data-science problems not covered in class, e.g., large datasets, limited computing resources, and changing requirements.  Personally, this project was also motivation for me to explore the features of R Markdown that I have not used in the past.

The key steps are importing the dataset, creating and tuning a model, and evaluating the model against an "unknown" dataset.  The raw data was imported using the provided script.  This script creates an edx dataset used to build a model and a validation dataset used for final evaluation.  I split the edx dataset into a train_set and a test_set in order to avoid making any modelling decisions with the validation set.  Then model parameters are tuned and evaluated against the test_set.  Finally, the model makes predictions for the users and movies in the previously unseen validation set to obtain the final RMSE score of 0.8606 meeting the requirement.

## Methods
The data required no cleaning.  There were no missing values and all existing values appeared reasonable.  I depended heavily on prior published work with the dataset (*The BellKor Solution to the Netflix Grand Prize*, Yehuda Koren, August 2009) to guide additional predictor creation and visualization. 

I choose an ensemble model with three sub-models: a very simplified version of BellKor Netflix solution (referred to as the "custom" built model) that extends the model developed during class, a tree-based gradient boosting model (XGBoost), and a nerual network model (NNET).  I did not have the computing resources to analyze the entire data.  I had to work with various sample sizes.  For example, in order to tune parameters for XGBoost and NNET I used a random sample of 1 million observations.  I assume that the sample represents the full dataset well enough to result in a reasonably good choice of parameters.

## Results
The ensemble pruduced predictions with an RMSE of 0.8606.  I was able to achieve an RMSE of 0.8606 which meets the required RMSE using only the model provided in class.  However, since my goal was to explore and practice I implemented the full ensemble of three.  The XGBoost and NNET models performed only slightly better than the custom model:

| Model | RMSE |
|-------|------|
| Ensemble | 0.8606 |
| Custom | 0.8623 |
| XGBoost | 0.8609 |
| NNet | 0.8608 |

This surprised me despite Bell and Koren's concluding remarks on baseline predictors:

> Out of the numerous new algorithmic contributions, I would like to highlight one â€“ those humble baseline predictors (or biases), which capture main effects in the data. While the literature mostly concentrates on the more sophisticated algorithmic aspects, we have learned that an accurate treatment of main effects is probably at least as significant as coming up with modeling breakthroughs.

## Conclusion
In conclusion, I was able to successfully create a model that meets the RMSE requirement.  I learned a lot during this project:

 * How to work with large dataset (sampling, model parameter guessing).  The 10 million records in the dataset are much too large for me to work with directly when exploring the data or developing the model.  Choosing smaller samples (from 10,000 to 1 million) was critical.
 
 * Microsoft Azure virtual machines.  My home computer was fine during the course.  But, it is unable to create the edx or validation sets.  My only option was to create a more powerful virtual machine - I choose Microsoft's Azure environment.  My final virtual machine had 8 processors and 32 GB of memory.  More powerful machines were available but their cost was prohibitive.
 
 * Parallel processing in R (`doParallel` package).  I learned that R does have the packages to support multiple CPUs.  This was especially useful during Caret training to determine the optimum model parameters.  Evaluating parameters went from 10-15 hours to 2-3 hours.
 
 * Finally, I enhanced my knowledge of Git distributed version control.  Using the Atlassian Sourcetree frontend for Git along with Github was essential for me to complete this project.  I could not have effectively worked on the same codebase using my physical home PC and the Azure VM without a central respository.  Not only did it allow me access from several computers, it saved me countless times by allowing me to revert to previous code versions after a long wlk down a dead-end development path.

There are several improvements to consider for improvements.  I would include the movie Genre as a predictor.  Perhaps as a component of the movie bias but also perhaps as a component to the user bias.  I would implement user bias as a function of time as in the Netflix competition model.  Finally, I would consider the number of movies a user reviews together (also part of the Netflix model) as a possible predictor.

A detailed exploratory data analysis follows.



```{r echo=FALSE, results='hide', message = FALSE}

library(tidyverse)
library(ggthemes)
library(lubridate)
library(doParallel)

# setup for parallel processing 
cl <- makePSOCKcluster(6)
registerDoParallel(cl)

edx <- readRDS(file = "./rda/edx.rds")

```

***
# Data Exploration
## Summary
The data is from the GroupLens research lab at the University of Minnesota.  The data consists of **`r nrow(edx)`** observations of **`r length(edx)`** variables. The data spans the years from **`r year(min(as_datetime(edx$timestamp)))`** to **`r year(max(as_datetime(edx$timestamp)))`**.

The `userId` variable is an integer representation of an actual MovieLens userId which has been anonymized to protect privacy.  The `movieId` is the actual MovieLens ID.  Ratings are made on a 5-star scale with half-star increments.  Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.  The title includes the title and year the movie was released.  The `genres` variable is a concatenation of all genres which apply to a specific movie.
```{r echo = FALSE}

summary(edx)
head(edx)

```

## Missing data, label encoding, and factorizing variables
Do any variables contain missing values?
```{r echo = FALSE}

NAcol <- which(colSums(is.na(edx)) > 0)

```
There are `r length(which(colSums(is.na(edx)) > 0))` columns with at least one NA.

There are no labels that are ordinal and should be encoded as a number.

We will convert `userId` and `movieId` from an integer to a factor.  We'll also separate the `genres` and store this result in a new dataframe - we'll need to refer to this dataframe a couple of times and don't want to redo this computationally expensive process.  We'll create a `review_date` variable from `timestamp` for the same reason.
```{r echo = FALSE}

edx <- edx %>%
  mutate(userId = factor(userId),
         movieId = factor(movieId),
         review_date = date(as_datetime(timestamp)))

edx_100k <- sample_n(edx, 100000)

movieId_by_genre <- edx %>%
  select(movieId, rating, genres) %>%
  separate_rows(genres, sep = "\\|")

num_genres <- length(unique(movieId_by_genre$genres))

```

## Response Variable
```{r echo = FALSE}

summary(edx$rating)

```
The `rating` response variable shows a predominance of whole-star ratings.  This may be due at least two factors: the rating data does not include half-star ratings until 2003 and users may have a strong preference for whole-star ratings.  Looking at either the whole-star or half-star ratings we see a somewhat normal distribution which is strongly left skewed.  This shows a slight user bias to rate movies higher than the midpoint of the scale (2.75).
```{r echo = FALSE, message = FALSE}

edx %>% ggplot(aes(x=rating)) +
  geom_histogram(binwidth = .5, colour = "white", fill = "#1380A1") +
  geom_hline(yintercept = 0, size = 1, colour="#333333") +
  scale_x_continuous(breaks = seq(.5, 5, .5)) +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Rating") +
  theme_economist_white()

```

Prior to May, 2003, there are no 1/2-star ratings.  But, this does not appear to effect the rating distribution.
It may be useful in a model as if a predicted rating is prior to May, 2003.  For example, if a model predicts
3.8 for a movie reviewed before May 2003 it may be better to round this to 4 stars.  Of course, if the actual rating is 3 stars then rounding will hurt.  So, this idea needs some experimentation.
```{r, echo = FALSE, message = FALSE}

edx_100k %>% 
  ggplot(aes(x = review_date, y = rating)) +
    geom_point() +
    labs(title = "Ratings", subtitle = "Random sample of 100k ratings") +
  theme_economist_white()

q <- edx %>% 
  mutate(half_star = if_else(review_date > ymd("2003-05-01"), TRUE, FALSE))

q %>% ggplot(aes(x = half_star, y = rating, fill = half_star)) +
    geom_boxplot() +
    labs(title = "Early Ratings vs Modern Ratings", subtitle = "No real difference") +
    scale_fill_manual(values = c("#990000", "#588300")) +
    scale_x_discrete(name = NULL, labels = c("Half-Stars Not Allowed", "Half-Stars OK")) +
    theme_economist_white() +
    theme(legend.position = "none")

```

## Other Variables
In the dataset there are:

- **`r num_genres`** unique movie genres.
- **`r length(unique(edx$userId))`** unique users
- **`r length(unique(edx$movieId))`** unique movies rated

### Genres
Number of reviewed movies in each genre.  Note that a movie may be tagged with several genres, e.g., the movie "Boomerang" is classified as both a "Comedy" and a "Romance."
```{r echo = FALSE}

movieId_by_genre %>%
group_by(genres) %>%
summarise(count = n()) %>%
ggplot() +
  geom_col(aes(x = reorder(genres, -count), y = count), fill = "#1380A1", color = "white") +
  coord_flip() +
  xlab("Genre") +
  ylab("Number of Movies") +
  labs(title = "Movies in Each Genre") +
  scale_y_continuous(labels = scales::comma) +
  theme_economist_white() +
  theme(axis.text.y.left = element_text(hjust = 1),
        panel.grid.major = element_blank(),
        axis.ticks.y.left = element_line(),
        axis.ticks.x.bottom = element_blank())

```

Which movies did not have a genre listed? 
```{r echo = FALSE}

edx %>% filter(genres == "(no genres listed)")

```

### Users
Movies Reviewed by Users, truncated to show only the left side of the extremely right-skewed distribution.
```{r echo = FALSE, message = FALSE}

q <- edx %>% 
  group_by(userId) %>% 
  summarise(num_of_movies_reviewed = n())

q %>% ggplot() +
  stat_count(geom = "bar", aes(x = num_of_movies_reviewed), color = "white", fill = "#1380A1") +
  labs(title = "User Activity", subtitle = "Most users review few movies") +
  geom_hline(yintercept = 0, size = 1, colour="#333333") +
  scale_x_continuous(name = "Number of Movies Reviewed by a User", limits = c(0, 300), labels = scales::comma) +
  scale_y_continuous(name = "Count of Users", labels = scales::comma) +
  theme_economist_white()
  
```

### Movies
How are the reviews/movie distributed?
```{r echo = FALSE}

edx_working <- edx %>% 
  group_by(movieId) %>% 
  summarise(num_of_reviews_movie = n())

edx_working %>% ggplot() +
  stat_count(geom = "bar", aes(x = num_of_reviews_movie), fill = "#1380A1") +
  labs(title = "Movie Popularity", subtitle = "Number of Reviews/Movie") +
  scale_x_continuous(name = "Number of Reviews per Movie", limits = c(0, 1000), labels = scales::comma) +
  scale_y_continuous(name = "Count of Movies", labels = scales::comma) +
  theme_economist_white()
  
```

What are the most frequently reviewed movies...
```{r echo = FALSE}

edx_working <- edx %>% 
    group_by(title) %>% 
    summarise(count = n()) %>% 
    arrange(-count)

edx_working[1:10,]

```
 ...and least frequently reviewed movies?
```{r echo = FALSE}

edx_working <- edx_working %>% 
  arrange(count)

edx_working[1:10,]

```

Its likely that not all users will rate movies the same.  Some users will generally rate high and others generally low.  This is called `user_bias` and is the difference between the average rating and this particular user's rating.  We can look for this bias by exploring the distribution of user ratings deviation from the mean of the population. We do the same for each movie - some movies are generally rated better than other movies - to establish a `movie_bias`.  We see both of these are fairly normally distributed.
```{r echo = FALSE}

mu <- mean(edx$rating)

edx %>% 
  group_by(userId) %>% 
  summarise(user_bias = mean(rating - mu)) %>% 
  ggplot() +
    geom_density(aes(x = user_bias), fill = "#1380A1") +
    theme_economist_white()

```

```{r echo = FALSE}

mu <- mean(edx$rating)

edx %>% 
  group_by(movieId) %>% 
  summarise(movie_bias = mean(rating - mu)) %>% 
  ggplot() +
    geom_density(aes(x = movie_bias), fill = "#1380A1") +
    labs(title = "Movie Bias") +
    theme_economist_white()

```

### Timestamp
```{r echo = FALSE}

summary(edx$review_date)

```

Does the overall average movie rating change over time?
```{r echo = TRUE, message = FALSE}

mu <- mean(edx$rating)

edx_working <- edx %>% 
  mutate(review_period = round_date(review_date, unit = "year")) %>% 
  group_by(review_period) %>% 
  summarise(avg_rating = mean(rating))

edx_working %>% ggplot(aes(x = review_period, y = avg_rating)) +
  geom_point() +
  geom_smooth(se = FALSE, colour = "#1380A1", size = 1) +
  scale_x_date(name = "Year") +
  labs(title = "Average Movie Rating Over Time") +
  theme_economist_white()

```

Does a particular movie's `average rating` change over time?  There are too many movies to visualize this effectively so we will select a small random sample.  For reference, the dashed horizontal line is the overall average rating.

```{r echo = FALSE, results = "hide", message = FALSE}

mu <- mean(edx$rating)

t <- sample(edx$movieId, 25)

edx_working <- edx %>% 
  filter(movieId %in% t) %>% 
  mutate(review_period = round_date(review_date, unit = "year")) %>% 
  group_by(movieId, review_period) %>% 
  summarise(avg_rating = mean(rating))

edx_working %>% 
  ggplot(aes(x = review_period, y = avg_rating)) +
    geom_line(colour = "#1380A1", size = 1) +
    geom_hline(yintercept = mu, color = "grey", linetype = "dashed") +
    scale_x_date(name = "Year") +
    scale_y_continuous(breaks = NULL) +
    labs(title = "Average Rating for Individual Movies Over Time") +
    facet_wrap(~ movieId) 
    
```


***
## Possible Predictors to Create
How does rating change related to the `movie's age` at the time of the review?  For reference, the dashed horizontal line is the overall average rating.  There are several data points that show reviews **before** the release date.  Perhaps these are bad data points or were reviewed by critics prior to being released.
```{r echo = FALSE, message = FALSE}

mu <- mean(edx$rating)

edx_working <- edx %>%
  mutate(movie_release_year = as.integer(str_extract(str_extract(title, "\\(\\d{4}\\)"), "\\d{4}")),
         movie_age_at_rating = year(review_date) - movie_release_year) 

edx_working <- edx_working %>% 
 group_by(movie_age_at_rating) %>%
 summarise(avg_rating = mean(rating))

edx_working %>% 
  ggplot(aes(x = movie_age_at_rating, y = avg_rating)) +
    geom_point(color = "grey") +
    geom_smooth(se = FALSE, color = "#1380A1") +
    geom_hline(yintercept = mu, linetype = "dashed") +
    theme_economist_white()

```

How does rating change related to `user's length of time reviewing` at the time of the review?
```{r echo = FALSE, message = FALSE}

mu <- mean(edx$rating)

t <- edx %>% 
  group_by(userId) %>% 
  summarise(user_first_rating = min(review_date))

edx_working <- edx %>% 
  left_join(t, by = "userId") %>% 
  mutate(days_since_first_rating =  as.integer(review_date - user_first_rating))

edx_working <- edx_working %>% 
  group_by(days_since_first_rating) %>%
  summarise(avg_rating = mean(rating))

edx_working %>% 
  ggplot(aes(x = days_since_first_rating, y = avg_rating)) +
    geom_point(color = "#dddddd") +
    geom_smooth(se = FALSE, color = "#1380A1") +
    scale_x_continuous(labels = scales::comma) +
    theme_economist_white()

```


Is there a relationship between the `number of reviews a movie received` (popularity) and the rating of that movie?  For reference, the dashed horizontal line is the overall average rating.
```{r echo = FALSE, message = FALSE}

mu <- mean(edx$rating)

t <- edx %>% 
  group_by(movieId) %>% 
  summarise(num_of_reviews_of_movie = n())

edx_working <- edx %>%
  left_join(t, by = "movieId")

edx_working <- edx_working %>% 
  group_by(num_of_reviews_of_movie) %>%
  summarise(avg_rating = mean(rating))

edx_working %>% 
  ggplot(aes(x = num_of_reviews_of_movie, y = avg_rating)) +
    geom_point(color = "grey") +
    geom_smooth(se = FALSE, color = "#1380A1") +
#    geom_hline(yintercept = mu, linetype = "dashed") +
    scale_x_continuous(labels = scales::comma) +
    theme_economist_white()

```

How does the number of movies reviewed by a user on a particular date (frequency) effect rating?
```{r echo = FALSE, message = FALSE}

edx_working <- edx

q <- edx_working %>% 
  group_by(userId, review_date) %>% 
  summarise(num_movies_reviewed_by_userId_today = n())

edx_working <- edx_working %>%  
  left_join(q)

edx_working %>% 
  ggplot(aes(x = num_movies_reviewed_by_userId_today, y = rating)) +
  geom_smooth(size = 1, color = "#1380A1") +
  scale_y_continuous(limits = c(0.5, 5)) +
  theme_economist_white()

```

## Proposed final predictors
```{r echo = FALSE}

edx_working <- edx

# create a reviewed_date and reviewed_year feature
edx_working <- edx_working %>%
  mutate(review_date = date(as_datetime(timestamp)),
         review_year = year(review_date))

mu <- mean(edx_working$rating)


# find b_i(t), the rating bias for each movie as a function of time (year)
lambda1 <- 3
movie_time_bias <- edx_working %>% 
  group_by(movieId, review_year) %>% 
  summarize(b_i_time = sum(rating - mu) / (n() + lambda1))
edx_working <- left_join(edx_working, movie_time_bias, by = c("movieId", "review_year"))

# find the user rating bias for each user
lambda2 <- 5
user_bias <- edx_working %>% 
  group_by(userId) %>% 
  summarize(b_u = sum(rating - mu - b_i_time) / (n() + lambda2) )
edx_working <- left_join(edx_working, user_bias, by = "userId")



# create released_year and movie_age_at_rating features
edx_working <- edx_working %>%
  mutate(movie_release_year = as.integer(str_extract(str_extract(title, "\\(\\d{4}\\)"), "\\d{4}")),
         movie_age_at_rating = year(review_date) - movie_release_year)


# Create the number of times a movie has been reviewed predictor
q <- edx_working %>% 
  group_by(movieId) %>% 
  summarise(num_of_reviews_of_movie = n())

edx_working <- edx_working %>%
  left_join(q, by = "movieId")



rm(lambda1, lambda2, q)

```

## Pairwise correlations amoung proposed predictors
```{r echo = FALSE}

edx_matrix <- edx_working %>% 
  select(rating, b_i_time, b_u, review_year, movie_age_at_rating, num_of_reviews_of_movie, movie_release_year) %>% 
  as.matrix()

c <- cor(edx_matrix)

corrplot::corrplot(c, method = "number", type = "lower")

```
First thing we notice is that the two strongest predicors of rating are the item and user biases.  They don't correlate with each other so we will use both of them in any model.  We see that the proposed predicotrs `movie_release_year` and `movie_age_at_rating` are strongly negatively correlated.  This makes sense as movies released in years far in the past are older!  Neither is better correlated with `rating` so we'll use `movie_age_at_rating` since it's a positive correlation and to me it seems more intuitive.

We'll use all proposed predictors except `review_year` and `movie_release_year` in our models.

```{r echo = FALSE, results = "hide"}

#Clean up

rm(edx_working, q, mu, NAcol, t)
gc()

```



