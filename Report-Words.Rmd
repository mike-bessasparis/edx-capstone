---
title: "Data Science: Capstone"
output: html_notebook
---

## Executive Summary
The explicit goal of project is to accurately predict the rating users will give to movies.  This is really motivation to reinforce past learning, find solutions to real-world data-science problems not covered in class, e.g., large datasets, limited computing resources, and changing requirements.  This project was also motivation to explore the features of R Markdown.

The key steps are importing the dataset, creating and tuning a model, and evaluating the model against an "unknown" dataset.  The raw data was imported using the provided script.  This script creates an edx dataset used to build a model and a validation dataset used for final evaluation.  I split the edx dataset into a train_set and a test_set in order to avoid making any modelling decisions with the validation set.  Then model parameters are tuned and evaluated against the test_set.  Finally, the model makes predictions for the users and movies in the previously unseen validation set to obtain the final RMSE score.

## Methods
a methods/analysis section that explains the process and techniques used, such as data cleaning, data exploration and visualization, any insights gained, and your modeling approach

The data required no cleaning.  There were no missing values and all existing values appeared reasonable.  I depended heavily on prior published work with the dataset (*The BellKor Solution to the Netflix Grand Prize*, Yehuda Koren, August 2009) to guide additional predictor creation and visualization. 

I choose an ensemble model with three sub-models: a very simplified version of BellKor Netflix solution (referred to as the "custom" built model) that extends the model developed during class, a tree-based gradient boosting model (XGBoost), and a nerual network model (NNET).  I did not have the computing resources to analyze the entire data.  I had to work with various sample sizes.  For example, in order to tune parameters for XGBoost and NNET I used a random sample of 1 million observations.  I assume that the sample is representative of the full dataset well enough to result in a reasonably good choice of parameters.

## Results
The ensemble pruduced predictions with an RMSE of 0.8606.  I was able to achieve an RMSE of 0.8606 which meets the required RMSE using only the model provided in class.  However, since my goal was to explore and practice I implemented the full ensemble of three.  The XGBoost and NNET models performed only slightly better than the custom model:

| Model | RMSE |
|-------|------|
| Custom | 0.8623 |
| XGBoost | 0.8609 |
| NNet | 0.8608 |
| Ensemble | 0.8606 |

This surprised me despite Bell and Koren's concluding remarks on baseline predictors:

> Out of the numerous new algorithmic contributions, I would like to highlight one â€“ those humble baseline predictors (or biases), which capture main effects in the data. While the literature mostly concentrates on the more sophisticated algorithmic aspects, we have learned that an accurate treatment of main effects is probably at least as significant as coming up with modeling breakthroughs.

## Conclusion
In conclusion, I was able to successfully create a model that meets the RMSE requirement.  I learned a lot during this process:

 * How to work with large dataset (sampling, model parameter guessing).  The 10 million records in the dataset are much too large for me to work with directly when exploring the data or developing the model.  Choosing smaller samples (from 10,000 to 1 million) was critical.
 
 * Microsoft Azure virtual machines.  My home computer was fine during the course.  But, it is unable to create the edx or validation sets.  My only option was to create a more powerful virtual machine - I choose Microsoft's Azure environment.  My final virtual machine had 8 processors and 32 GB of memory.  More powerful machines were available but their cost was prohibitive.
 
 * Parallel processing in R (`doParallel` package).  I learned that R does have the packages to support multiple CPUs.  This was especially useful during Caret training to determine the optimum model parameters.  Evaluating parameters went from 10-15 hours to 2-3 hours.
 
 * Finally, I enhanced my knowledge of Git distributed version control.  Using the Atlassian Sourcetree frontend for Git along with Github was essential for me to complete this project.  I could not have effectively worked on the same codebase using my physical home PC and the Azure VM without a central respository.  Not only did it allow me access from several computers, it saved me countless times by allowing me to revert to previous code versions after a long wlk down a dead-end development path.

The detailed exploratory data analysis follows.


